# RunPod Training Configuration for Clearledgr Llama 3.1 8B

# Model Configuration
model:
  name: "meta-llama/Llama-3.1-8B"
  max_length: 2048
  use_4bit_quantization: true
  use_flash_attention: true

# LoRA Configuration
lora:
  enabled: true
  rank: 16
  alpha: 32
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "k_proj" 
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  modules_to_save:
    - "embed_tokens"
    - "lm_head"

# Training Configuration
training:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 100
  
  # Optimization
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 4
  
  # Logging and Saving
  logging_steps: 100
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  
  # Early stopping
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Dataset Configuration  
dataset:
  train_split: 0.9
  validation_split: 0.1
  seed: 42

# RunPod Optimization
runpod:
  # GPU settings for A100 40GB
  gpu_memory_fraction: 0.95
  mixed_precision: "fp16"
  
  # Monitoring
  monitor_gpu_usage: true
  monitor_memory_usage: true
  save_training_plots: true
  
  # Checkpointing
  resume_from_checkpoint: true
  checkpoint_every_n_steps: 500

# Weights & Biases Configuration
wandb:
  enabled: true
  project: "clearledgr-llama"
  entity: "clearledgr-ai"
  tags:
    - "llama-3.1-8b"
    - "financial-ai"
    - "lora"
    - "runpod"

# Output Configuration
output:
  base_dir: "./models/clearledgr-llama-3.1-8b"
  save_merged_model: true
  save_training_config: true
  create_model_card: true
